{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Intro to neurons\n\nAt this point, we know how to build models and have a computer automatically learn how to match the model to data. This is the core of how any machine learning method works. \n\nNow, let's narrow our focus and look at **neural networks**. Neural networks (or \"neural nets\", for short) are a specific choice of a **model**. It's a network made up of **neurons**; this, in turn, leads to the question, \"what is a neuron?\""
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models with multiple inputs\n\nSo far, we have been using the sigmoid function as our model. One of the forms of the sigmoid function we've used is\n\n$$\\sigma_{w, b}(x) = \\frac{1}{1 + \\exp(-wx + b)},$$\n\nwhere `x` and `w` are both single numbers. We have been using this function to model how the amount of the color green in an image (`x`) can be used to determine if an image shows an apple or a banana. \n\nBut what if we had multiple data features we wanted to fit? \n\nWe could then extend our model to include multiple features like\n\n$$\\sigma_{\\mathbf{w},b}(\\mathbf{x}) = \\frac{1}{1 + \\exp(-w_1 x_1 - w_2 x_2 - \\cdots - w_n x_n + b)}$$\n\nNote that now $\\mathbf{x}$ and $\\mathbf{w}$ are vectors with many components, rather than a single number. \n\nFor example, $x_1$ might be the amount of the color green in an image, $x_2$ could be the height of the object in the picture, $x_3$ could be the width, and so forth. We can add information for as many features as we have! Our model now has more parameters, but the same idea of gradient descent (\"rolling the ball downhill\") will still work to train our model.\n\nThis version of the sigmoid model that takes multiple inputs is an example of a **neuron**."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the video, we see that one huge class of learning techniques is based around neurons, that is, *artificial neurons*. These are caricatures of real, biological neurons. Both *artificial* and *biological* neurons have several inputs $x_1, \\ldots, x_n$, and a single output, $y$. Schematically they look like this:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "include(\"draw_neural_net.jl\")\n\nnumber_inputs, number_neurons = 4, 1\n\ndraw_network([number_inputs, number_neurons])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should read this as showing how information flows from left to right: \n- 4 pieces of input information arrive (shown in green on the left);\n\n- the neuron (shown in red on the right) receives all the inputs, processes them, and outputs a single number to the right."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In other words, a neuron is just a type of function that takes multiple inputs and returns a single output.\n\nThe simplest interesting case that we will look at in this notebook is when there are just two pieces of input data:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "draw_network([2, 1])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each link between circles above represents a **weight** $w$ that can be modified to allow the neuron to learn, so in this case the neuron has two weights, $w_1$ and $w_2$. \n\nThe neuron also has a single bias $b$, and an **activation function**, which we will take to be the $\\sigma$ sigmoidal function that we have been using. (Note that other activation functions can be used!)\n\nLet's call our neuron $f_{w_1,w_2, b}(x_1, x_2)$, where\n\n$$f_{w_1,w_2, b}(x_1, x_2) := \\sigma(w_1 x_1 + w_2 x_2 + b).$$\n\nNote that $f_{w_1,w_2, b}(x_1, x_2)$ has 3 parameters: two weights and a bias.\n\nTo simplify the notation, and to prepare for more complicated scenarios later, we put the two weights into a vector, and the two data values into another vector:\n\n$$\n\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix};\n\\qquad\n\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}.\n$$\n\nWe thus have\n\n$$f_{\\mathbf{w}, b}(\\mathbf{x}) = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b),$$\n\nwhere the dot product $\\mathbf{w} \\cdot \\mathbf{x}$ is an abbreviated syntax for $w_1 x_1 + w_2 x_2$."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1\n\nDeclare the function `f(x, w, b)` in Julia. `f` should take vectors `x` and `w` as vectors and `b` as a scalar. Furthermore `f` should call\n\n```julia\nÏƒ(x) = 1 / (1 + exp(-x))\n```\n\nWhat output do you get for\n\n```julia\nf(3, 4, 5)\n```\n?"
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "0.6.2"
    },
    "kernelspec": {
      "name": "julia-0.6",
      "display_name": "Julia 0.6.2",
      "language": "julia"
    }
  },
  "nbformat": 4
}
