{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple neural network layers with `Flux.jl`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a previous notebook, we saw that one layer of neurons wasn't enough to distinguish between three types of fruit (apples, bananas *and* grapes), since the data is quite complex. To solve this problem, we need to use more layers, so heading into the territory of **deep learning**!\n\nBy adding another layer between the inputs and the output neurons, a so-called \"hidden layer\", we will get our first serious **neural network**, looking something like this:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "include(\"draw_neural_net.jl\")\ndraw_network([2, 4, 3])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will continue to use two input data and try to classify into three types, so we will have three output neurons. We have chosen to add a single \"hidden layer\" in between, and have arbitrarily chosen to put 4 neurons there.\n\nMuch of the *art* of deep learning is choosing a suitable structure for the neural network that will allow the model to be sufficiently complex to model the data well, but sufficiently simple to allow the parameters to be learned in a reasonable length of time."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read in and process data\n\nAs before, let's load some pre-processed data using code we've seen in the previous notebook."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Flux\nusing Flux: onehot"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using CSV\n\napples_1 = CSV.read(\"data/Apple_Golden_1.dat\", delim='\\t')\napples_2 = CSV.read(\"data/Apple_Golden_2.dat\", delim='\\t')\napples_3 = CSV.read(\"data/Apple_Golden_3.dat\", delim='\\t')\nbananas = CSV.read(\"data/Banana.dat\", delim='\\t')\ngrapes_1 = CSV.read(\"data/Grape_White.dat\", delim='\\t')\ngrapes_2 = CSV.read(\"data/Grape_White_2.dat\", delim='\\t');\n\napples = vcat(apples_1, apples_2, apples_3)\ngrapes = vcat(grapes_1, grapes_2);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "col1 = :red\ncol2 = :blue\n\nx_apples  = [ [apples_1[i, col1], apples_1[i, col2]] for i in 1:size(apples_1)[1] ]\nappend!(x_apples, [ [apples_2[i, col1], apples_2[i, col2]] for i in 1:size(apples_2)[1] ])\nappend!(x_apples, [ [apples_3[i, col1], apples_3[i, col2]] for i in 1:size(apples_3)[1] ])\n\nx_bananas = [ [bananas[i, col1], bananas[i, col2]] for i in 1:size(bananas)[1] ]\n\nx_grapes = [ [grapes_1[i, col1], grapes_1[i, col2]] for i in 1:size(grapes_1)[1] ]\nappend!(x_grapes, [ [grapes_2[i, col1], grapes_2[i, col2]] for i in 1:size(grapes_2)[1] ])\n\nxs = vcat(x_apples, x_bananas, x_grapes);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now we wish to classify the three types of fruit, so we again use one-hot vectors to represent the desired outputs $y^{(i)}$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "labels = [ones(length(x_apples)); 2*ones(length(x_bananas)); 3*ones(length(x_grapes))];\n\nys = [onehot(label, 1:3) for label in labels];  # onehotbatch(labels, 1:3)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input data is in `xs` and the one-hot vectors are in `ys`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple layers in Flux"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's tell Flux what structure we want the network to have. We first specify the number of neurons in each layer, and then construct each layer as a `Dense` layer:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "inputs = 2\nhidden = 4\noutputs = 3\n\nlayer1 = Dense(inputs, hidden, σ)\nlayer2 = Dense(hidden, outputs, σ)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To stitch together multiple layers to make a multi-layer network, we use Flux's `Chain` function:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "model = Chain(layer1, layer2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1\n\nWhat is the internal structure and sub-structure of this `model` object?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now set up a model and we have some training data.\nHow do we train the model on the data?\n    \nThe amazing thing is that the rest of the code in `Flux` is **exactly the same as before**. This is possible thanks to the design of Julia itself, and of the `Flux` package."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 2\n\nTrain the model as before, now using the popular `ADAM` optimizer. You may need to train the network for longer than before, since we have many more parameters."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the results"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does this neural network represent? It is simply a more complicated function with two inputs and three outputs, i.e. a function $f: \\mathbb{R}^2 \\to \\mathbb{R}^3$. \nBefore, with a single layer, each component of the function $f$ basically corresponded to a hyperplane; now it will instead be a **more complicated nonlinear function** of the input data!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 3\n\nVisualize each component of the output separately as a heatmap and/or contours superimposed on the data. Interpret the results."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What we have learned"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding an intermediate layer allows the network to start to deform the separating surfaces that it is learning into more complicated, nonlinear (curved) shapes. This allows it to separate data that were previously unable to be separated!\n\nHowever, using only two features means that data from different classes overlaps. To distinguish it we would need to use more features."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4\n\nUse three features (red, green and blue) and build a network with one hidden layer. Does this help to distinguish the data better?"
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "0.6.2"
    },
    "kernelspec": {
      "name": "julia-0.6",
      "display_name": "Julia 0.6.2",
      "language": "julia"
    }
  },
  "nbformat": 4
}
